import { Injectable, Logger } from '@nestjs/common';
import { LlmService } from './llm.service';

@Injectable()
export class OllamaService extends LlmService {
  private readonly logger = new Logger(OllamaService.name);
  
  async generate(prompt: string): Promise<string> {
    this.logger.log('Generating with Ollama...');
    try {
        // Basic Ollama integration
        const url = process.env.OLLAMA_API_URL || 'http://localhost:11434/api/generate';
        const response = await fetch(url, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                model: 'llama3', 
                prompt: prompt,
                stream: false
            })
        });
        
        if (!response.ok) {
            throw new Error(`Ollama responded with ${response.status}`);
        }
        
        const data = await response.json();
        return data.response;
    } catch (e) {
        this.logger.warn(`Ollama error: ${e.message}. Returning mock content.`);
        return `
# Draft Content
(Generated by Mock due to LLM error)

## Introduction
This is a draft content for the prompt.

## Body
Content body here.

## Conclusion
Conclusion here.
        `;
    }
  }
}
